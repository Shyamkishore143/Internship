{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95086473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\my pc\\anaconda3\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: requests in c:\\users\\my pc\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from requests) (2022.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 pandas requests\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17c62c30",
   "metadata": {},
   "source": [
    "\n",
    "Question 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07286af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_wikipedia_header_tags(url):\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all header tags in the HTML content\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    # Extract the text from the header tags\n",
    "    header_text = [tag.get_text().strip() for tag in header_tags]\n",
    "\n",
    "    # Create a pandas DataFrame with the header text\n",
    "    df = pd.DataFrame(header_text, columns=['Header'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1f29da7",
   "metadata": {},
   "source": [
    "\n",
    "Quetion 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b862917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_imdb_top_50_movies(url):\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the movie containers in the HTML content\n",
    "    movie_containers = soup.find_all('div', class_='lister-item mode-advanced')\n",
    "\n",
    "    # Create lists to store the movie details\n",
    "    movie_names = []\n",
    "    movie_ratings = []\n",
    "    movie_years = []\n",
    "\n",
    "    # Extract the movie details from the movie containers\n",
    "    for container in movie_containers:\n",
    "        # Extract the name of the movie\n",
    "        name = container.h3.a.text\n",
    "        movie_names.append(name)\n",
    "\n",
    "        # Extract the rating of the movie\n",
    "        rating = float(container.strong.text)\n",
    "        movie_ratings.append(rating)\n",
    "\n",
    "        # Extract the year of release of the movie\n",
    "        year = container.h3.find('span', class_='lister-item-year').text\n",
    "        year = int(year.replace('(', '').replace(')', ''))\n",
    "        movie_years.append(year)\n",
    "\n",
    "    # Create a pandas DataFrame with the movie details\n",
    "    df = pd.DataFrame({\n",
    "        'Name': movie_names,\n",
    "        'Rating': movie_ratings,\n",
    "        'Year': movie_years\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c5ad112",
   "metadata": {},
   "source": [
    "\n",
    "Quetion 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_imdb_top_50_indian_movies(url):\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the movie containers in the HTML content\n",
    "    movie_containers = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "    # Create lists to store the movie details\n",
    "    movie_names = []\n",
    "    movie_ratings = []\n",
    "    movie_years = []\n",
    "\n",
    "    # Extract the movie details from the movie containers\n",
    "    for container in movie_containers:\n",
    "        # Extract the name of the movie\n",
    "        name = container.h3.a.text\n",
    "        movie_names.append(name)\n",
    "\n",
    "        # Extract the rating of the movie\n",
    "        rating = float(container.strong.text)\n",
    "        movie_ratings.append(rating)\n",
    "\n",
    "        # Extract the year of release of the movie\n",
    "        year = container.h3.find('span',\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a79f9f3a",
   "metadata": {},
   "source": [
    "\n",
    "Quetion 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_former_presidents_data(url):\n",
    "    # Send a request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the data for former presidents\n",
    "    table = soup.find('table', {'class': 'tablepress tablepress-id-24'})\n",
    "    \n",
    "    # Extract the data from the table\n",
    "    data = []\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        name = cols[0].get_text().strip()\n",
    "        term_of_office = cols[1].get_text().strip()\n",
    "        data.append((name, term_of_office))\n",
    "    \n",
    "    # Create a dataframe from the extracted data\n",
    "    df = pd.DataFrame(data, columns=['Name', 'Term of Office'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "541287fb",
   "metadata": {},
   "source": [
    "\n",
    "Quetion 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e06f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_top10_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    table = soup.find_all('table')[0]\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    for row in rows[:10]:\n",
    "        team_name = row.find('span', class_='u-hide-phablet').text\n",
    "        teams.append(team_name)\n",
    "        matches_played = row.find_all('td')[2].text\n",
    "        matches.append(int(matches_played))\n",
    "        points_scored = row.find_all('td')[3].text\n",
    "        points.append(int(points_scored))\n",
    "        rating = row.find_all('td')[4].text.strip()\n",
    "        ratings.append(int(rating))\n",
    "\n",
    "    odi_teams_df = pd.DataFrame({\n",
    "        'Team Name': teams,\n",
    "        'Matches Played': matches,\n",
    "        'Points Scored': points,\n",
    "        'Ratings': ratings\n",
    "    })\n",
    "    return odi_teams_df\n",
    "\n",
    "def get_top10_odi_batsmen(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    table = soup.find_all('table')[0]\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    for row in rows[:10]:\n",
    "        player_name = row.find('div', class_='rankings-player__name').text.strip()\n",
    "        players.append(player_name)\n",
    "        team_name = row.find('div', class_='rankings-player__team').text.strip()\n",
    "        teams.append(team_name)\n",
    "        rating = row.find('div', class_='rankings-player__rating').text\n",
    "        ratings.append(int(rating))\n",
    "\n",
    "    odi_batsmen_df = pd.DataFrame({\n",
    "        'Player Name': players,\n",
    "        'Team Name': teams,\n",
    "        'Ratings': ratings\n",
    "    })\n",
    "    return odi_batsmen_df\n",
    "\n",
    "def get_top10_odi_bowlers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    table = soup.find_all('table')[1]\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    for row in rows[:10]:\n",
    "        player_name = row.find('div', class_='rankings-player__name').text.strip()\n",
    "        players.append(player_name)\n",
    "        team_name = row.find('div', class_='rankings-player__team').text.strip()\n",
    "        teams.append(team_name)\n",
    "        rating = row.find('div', class_='rankings-player__rating').text\n",
    "        ratings.append(int(rating))\n",
    "\n",
    "    odi_bowlers_df = pd.DataFrame({\n",
    "        'Player Name': players,\n",
    "        'Team Name': teams,\n",
    "        'Ratings': ratings\n",
    "    })\n",
    "    return odi_bowlers_df\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "odi_teams_df = get_top10_odi_teams(url)\n",
    "print('Top 10 ODI Teams')\n",
    "print(odi_teams_df)\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_batsmen_df = get_top10_odi_batsmen(url)\n",
    "print('\\nTop 10 ODI Batsmen')\n",
    "print(odi_batsmen_df)\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_bowlers_df = get_top10_odi_bowlers(url)\n",
    "print('\\nTop 10 ODI Bowlers')\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c77f86c",
   "metadata": {},
   "source": [
    "\n",
    "Quetion 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4848ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_womens_rankings_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Scrape top 10 women's ODI teams\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.tbody.find_all('tr')\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        teams.append(columns[1].text.strip())\n",
    "        matches.append(columns[2].text.strip())\n",
    "        points.append(columns[3].text.strip())\n",
    "        ratings.append(columns[4].text.strip())\n",
    "    \n",
    "    womens_teams_df = pd.DataFrame({\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "    \n",
    "    # Scrape top 10 women's ODI batting players\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.tbody.find_all('tr')\n",
    "    for row in rows[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        players.append(columns[1].text.strip())\n",
    "        teams.append(columns[2].text.strip())\n",
    "        ratings.append(columns[4].text.strip())\n",
    "    \n",
    "    womens_batting_df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "    \n",
    "    # Scrape top 10 women's ODI all-rounders\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.tbody.find_all('tr')\n",
    "    for row in rows[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        players.append(columns[1].text.strip())\n",
    "        teams.append(columns[2].text.strip())\n",
    "        ratings.append(columns[4].text.strip())\n",
    "    \n",
    "    womens_allrounders_df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "    \n",
    "    return womens_teams_df, womens_batting_df, womens_allrounders_df\n",
    "\n",
    "# Example usage\n",
    "womens_teams, womens_batting, womens_allrounders = get_womens_rankings_data('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "print('Top 10 ODI Women\\'s Teams')\n",
    "print(womens_teams)\n",
    "print('\\nTop 10 Women\\'s ODI Batting Players')\n",
    "print(womens_batting)\n",
    "print('\\nTop 10 Women\\'s ODI All-rounders')\n",
    "print(womens_allrounders)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ec33669",
   "metadata": {},
   "source": [
    "\n",
    "Quetion 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc13bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    # send a request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # find all news articles\n",
    "    articles = soup.find_all('div', {'class': 'Card-titleContainer'})\n",
    "\n",
    "    # create an empty list to store news data\n",
    "    news_data = []\n",
    "\n",
    "    # loop through each news article and extract headline, time and news link\n",
    "    for article in articles:\n",
    "        # get headline\n",
    "        headline = article.find('a', {'class': 'Card-titleLink'}).text.strip()\n",
    "\n",
    "        # get time\n",
    "        time = article.find('time', {'class': 'Card-time'}).text.strip()\n",
    "\n",
    "        # get news link\n",
    "        news_link = article.find('a', {'class': 'Card-titleLink'})['href']\n",
    "\n",
    "        # add data to the list\n",
    "        news_data.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "    # create a pandas DataFrame from the list\n",
    "    df = pd.DataFrame(news_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "df = scrape_cnbc_news(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d9af3b8",
   "metadata": {},
   "source": [
    "\n",
    "Question 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_titles = []\n",
    "    article_authors = []\n",
    "    article_publish_dates = []\n",
    "    article_urls = []\n",
    "\n",
    "    for article in soup.select('div[class=\"pod-listing-header\"]'):\n",
    "        article_titles.append(article.select_one('h4 > a').text.strip())\n",
    "        article_authors.append(article.select_one('span[class=\"text-silver-dark\"]').text.strip())\n",
    "        article_publish_dates.append(article.select_one('time').text.strip())\n",
    "        article_urls.append(article.select_one('h4 > a')['href'])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Paper Title': article_titles,\n",
    "        'Authors': article_authors,\n",
    "        'Published Date': article_publish_dates,\n",
    "        'Paper URL': article_urls\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# call the function and pass the URL as parameter\n",
    "df = get_most_downloaded_articles(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "\n",
    "# display the resulting data frame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af8264dc",
   "metadata": {},
   "source": [
    "\n",
    "Question 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# send a GET request to the URL\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# extract the required details\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "for restaurant in soup.find_all('div', class_='restnt-info'):\n",
    "    restaurant_names.append(restaurant.find('h2').text.strip())\n",
    "    cuisines.append(restaurant.find('p', class_='double-line-ellipsis').text.strip())\n",
    "    locations.append(restaurant.find('p', class_='restnt-loc ellipsis mb-0').text.strip())\n",
    "    ratings.append(restaurant.find('div', class_='rating-val').text.strip())\n",
    "    image_urls.append(restaurant.find('img')['src'])\n",
    "\n",
    "# create a dictionary from the extracted data\n",
    "data = {'Restaurant Name': restaurant_names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Rating': ratings,\n",
    "        'Image URL': image_urls}\n",
    "\n",
    "# create a pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8023160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
