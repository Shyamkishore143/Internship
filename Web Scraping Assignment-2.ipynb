{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5f46d810",
   "metadata": {},
   "source": [
    "Q1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_naukri_jobs(job_title, location, num_jobs):\n",
    "    url = 'https://www.naukri.com/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    params = {\n",
    "        'k': job_title,\n",
    "        'l': location\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    jobs = soup.find_all('article', {'itemtype': 'http://schema.org/JobPosting'})[:num_jobs]\n",
    "\n",
    "    for job in jobs:\n",
    "        job_titles.append(job.find('a', {'class': 'title'}).text.strip())\n",
    "        job_locations.append(job.find('li', {'class': 'location'}).text.strip())\n",
    "        company_names.append(job.find('a', {'class': 'subTitle'}).text.strip())\n",
    "        experience_required.append(job.find('li', {'class': 'experience'}).text.strip())\n",
    "\n",
    "    data = {\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the job title, location, and number of jobs to scrape\n",
    "job_title = 'Data Analyst'\n",
    "location = 'Bangalore'\n",
    "num_jobs = 10\n",
    "\n",
    "# Scrape the jobs data and create a DataFrame\n",
    "df = scrape_naukri_jobs(job_title, location, num_jobs)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b56a5871",
   "metadata": {},
   "source": [
    "Q2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_naukri_jobs(job_title, location, num_jobs):\n",
    "    url = 'https://www.naukri.com/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    params = {\n",
    "        'k': job_title,\n",
    "        'l': location\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "\n",
    "    jobs = soup.find_all('article', {'itemtype': 'http://schema.org/JobPosting'})[:num_jobs]\n",
    "\n",
    "    for job in jobs:\n",
    "        job_titles.append(job.find('a', {'class': 'title'}).text.strip())\n",
    "        job_locations.append(job.find('li', {'class': 'location'}).text.strip())\n",
    "        company_names.append(job.find('a', {'class': 'subTitle'}).text.strip())\n",
    "\n",
    "    data = {\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the job title, location, and number of jobs to scrape\n",
    "job_title = 'Data Scientist'\n",
    "location = 'Bangalore'\n",
    "num_jobs = 10\n",
    "\n",
    "# Scrape the jobs data and create a DataFrame\n",
    "df = scrape_naukri_jobs(job_title, location, num_jobs)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0565d579",
   "metadata": {},
   "source": [
    "Q3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_naukri_jobs(job_title, location, salary, num_jobs):\n",
    "    url = 'https://www.naukri.com/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    params = {\n",
    "        'k': job_title,\n",
    "        'l': location,\n",
    "        'salary': salary\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    jobs = soup.find_all('article', {'itemtype': 'http://schema.org/JobPosting'})[:num_jobs]\n",
    "\n",
    "    for job in jobs:\n",
    "        job_titles.append(job.find('a', {'class': 'title'}).text.strip())\n",
    "        job_locations.append(job.find('li', {'class': 'location'}).text.strip())\n",
    "        company_names.append(job.find('a', {'class': 'subTitle'}).text.strip())\n",
    "        experience_required.append(job.find('li', {'class': 'experience'}).text.strip())\n",
    "\n",
    "    data = {\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the job title, location, salary range, and number of jobs to scrape\n",
    "job_title = 'Data Scientist'\n",
    "location = 'Delhi/NCR'\n",
    "salary = '3-6'\n",
    "num_jobs = 10\n",
    "\n",
    "# Scrape the jobs data and create a DataFrame\n",
    "df = scrape_naukri_jobs(job_title, location, salary, num_jobs)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2aeb3cd",
   "metadata": {},
   "source": [
    "Q3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba70caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_naukri_jobs(job_title, location, salary, num_jobs):\n",
    "    url = 'https://www.naukri.com/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    params = {\n",
    "        'k': job_title,\n",
    "        'l': location,\n",
    "        'salary': salary\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    jobs = soup.find_all('article', {'itemtype': 'http://schema.org/JobPosting'})[:num_jobs]\n",
    "\n",
    "    for job in jobs:\n",
    "        job_titles.append(job.find('a', {'class': 'title'}).text.strip())\n",
    "        job_locations.append(job.find('li', {'class': 'location'}).text.strip())\n",
    "        company_names.append(job.find('a', {'class': 'subTitle'}).text.strip())\n",
    "        experience_required.append(job.find('li', {'class': 'experience'}).text.strip())\n",
    "\n",
    "    data = {\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the job title, location, salary range, and number of jobs to scrape\n",
    "job_title = 'Data Scientist'\n",
    "location = 'Delhi/NCR'\n",
    "salary = '3-6'\n",
    "num_jobs = 10\n",
    "\n",
    "# Scrape the jobs data and create a DataFrame\n",
    "df = scrape_naukri_jobs(job_title, location, salary, num_jobs)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0764f133",
   "metadata": {},
   "source": [
    "Q4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_sunglasses(num_listings):\n",
    "    url = 'https://www.flipkart.com/'\n",
    "    search_query = 'sunglasses'\n",
    "\n",
    "    # Search for sunglasses\n",
    "    response = requests.get(url, params={'q': search_query})\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the product URLs from the search results\n",
    "    product_urls = []\n",
    "    product_links = soup.find_all('a', {'class': 'IRpwTa'})\n",
    "    for link in product_links:\n",
    "        product_urls.append('https://www.flipkart.com' + link['href'])\n",
    "\n",
    "    # Scrape the required data from each product page\n",
    "    data = {\n",
    "        'Brand': [],\n",
    "        'Product Description': [],\n",
    "        'Price': []\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    for url in product_urls:\n",
    "        if count >= num_listings:\n",
    "            break\n",
    "\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        brand = soup.find('span', {'class': 'B_NuCI'}).text\n",
    "        product_desc = soup.find('span', {'class': '_2FZd5H'}).text\n",
    "        price = soup.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "\n",
    "        data['Brand'].append(brand)\n",
    "        data['Product Description'].append(product_desc)\n",
    "        data['Price'].append(price)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the number of listings to scrape\n",
    "num_listings = 100\n",
    "\n",
    "# Scrape the sunglasses data and create a DataFrame\n",
    "df = scrape_flipkart_sunglasses(num_listings)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee770014",
   "metadata": {},
   "source": [
    "Q5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e0b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating       Review Summary  \\\n",
      "0       5       Simply awesome   \n",
      "1       5     Perfect product!   \n",
      "2       5  Best in the market!   \n",
      "3       4      Value-for-money   \n",
      "4       5   Highly recommended   \n",
      "..    ...                  ...   \n",
      "95      5     Perfect product!   \n",
      "96      5   Highly recommended   \n",
      "97      4      Value-for-money   \n",
      "98      5   Highly recommended   \n",
      "99      5     Perfect product!   \n",
      "\n",
      "                                          Full Review  \n",
      "0   Really satisfied with the Product I received.....  \n",
      "1   Amazing phone with great cameras and better ba...  \n",
      "2   Great iPhone very snappy experience as apple k...  \n",
      "3   I'm Really happy with the productDelivery was ...  \n",
      "4   It's my first time to use iOS phone and I am l...  \n",
      "..                                                ...  \n",
      "95  Value for money❤️❤️Its awesome mobile phone in...  \n",
      "96  iphone 11 is a very good phone to buy only if ...  \n",
      "97  Just got this iphone 11 And it is most powerfu...  \n",
      "98  Amazing camera quality as expected, battery al...  \n",
      "99  It is just awesome mobile for this price from ...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_reviews):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    data = {\n",
    "        'Rating': [],\n",
    "        'Review Summary': [],\n",
    "        'Full Review': []\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    while count < num_reviews:\n",
    "        reviews = soup.find_all('div', {'class': '_27M-vq'})\n",
    "        for review in reviews:\n",
    "            if count >= num_reviews:\n",
    "                break\n",
    "\n",
    "            rating = review.find('div', {'class': '_3LWZlK _1BLPMq'}).text\n",
    "            summary = review.find('p', {'class': '_2-N8zT'}).text\n",
    "            full_review = review.find('div', {'class': 't-ZTKy'}).text\n",
    "\n",
    "            data['Rating'].append(rating)\n",
    "            data['Review Summary'].append(summary)\n",
    "            data['Full Review'].append(full_review)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        next_page_link = soup.find('a', {'class': '_1LKTO3'})\n",
    "        if next_page_link is None:\n",
    "            break\n",
    "\n",
    "        next_page_url = 'https://www.flipkart.com' + next_page_link['href']\n",
    "        response = requests.get(next_page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the URL and number of reviews to scrape\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "num_reviews = 100\n",
    "\n",
    "# Scrape the reviews data and create a DataFrame\n",
    "df = scrape_flipkart_reviews(url, num_reviews)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afdb3b92",
   "metadata": {},
   "source": [
    "Q6.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208df251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_sneakers(url, num_sneakers):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    data = {\n",
    "        'Brand': [],\n",
    "        'Product Description': [],\n",
    "        'Price': []\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    while count < num_sneakers:\n",
    "        sneakers = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "        for sneaker in sneakers:\n",
    "            if count >= num_sneakers:\n",
    "                break\n",
    "\n",
    "            brand = sneaker.find('div', {'class': '_2WkVRV'}).text\n",
    "            description = sneaker.find('a', {'class': 'IRpwTa'}).text\n",
    "            price = sneaker.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "\n",
    "            data['Brand'].append(brand)\n",
    "            data['Product Description'].append(description)\n",
    "            data['Price'].append(price)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        next_page_link = soup.find('a', {'class': '_1LKTO3'})\n",
    "        if next_page_link is None:\n",
    "            break\n",
    "\n",
    "        next_page_url = 'https://www.flipkart.com' + next_page_link['href']\n",
    "        response = requests.get(next_page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the URL and number of sneakers to scrape\n",
    "url = 'https://www.flipkart.com/search?q=sneakers'\n",
    "num_sneakers = 100\n",
    "\n",
    "# Scrape the sneakers data and create a DataFrame\n",
    "df = scrape_flipkart_sneakers(url, num_sneakers)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb2d1c85",
   "metadata": {},
   "source": [
    "Q7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_amazon_laptops(url, num_laptops):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    data = {\n",
    "        'Title': [],\n",
    "        'Ratings': [],\n",
    "        'Price': []\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    laptops = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    for laptop in laptops:\n",
    "        if count >= num_laptops:\n",
    "            break\n",
    "\n",
    "        title = laptop.find('span', {'class': 'a-size-medium a-color-base a-text-normal'}).text\n",
    "        ratings = laptop.find('span', {'class': 'a-icon-alt'}).text.split()[0]\n",
    "        price = laptop.find('span', {'class': 'a-price-whole'}).text.replace(',', '')\n",
    "\n",
    "        data['Title'].append(title)\n",
    "        data['Ratings'].append(ratings)\n",
    "        data['Price'].append(price)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Set the URL and number of laptops to scrape\n",
    "url = 'https://www.amazon.in/'\n",
    "search_query = 'Laptop'\n",
    "cpu_filter = 'Intel Core i7'\n",
    "num_laptops = 10\n",
    "\n",
    "# Set the search query and apply the CPU filter\n",
    "params = {\n",
    "    'k': search_query\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "}\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "cpu_filter_link = soup.find('a', {'href': '/s?k=' + search_query + '&rh=n%3A976392031%2Cp_n_feature_thirteen_browse-bin%3A12598161031'})\n",
    "if cpu_filter_link is None:\n",
    "    print('CPU Type filter not found.')\n",
    "else:\n",
    "    cpu_filter_url = 'https://www.amazon.in' + cpu_filter_link['href']\n",
    "    response = requests.get(cpu_filter_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Scrape the laptops data and create a DataFrame\n",
    "    df = scrape_amazon_laptops(url, num_laptops)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796160e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_quotes(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    quote_items = soup.find_all('div', class_='wrap-blockquote')\n",
    "\n",
    "    quotes = []\n",
    "    for item in quote_items:\n",
    "        quote = item.find('a', class_='title').text.strip()\n",
    "        author = item.find('a', class_='author').text.strip()\n",
    "        quote_type = item.find('a', class_='kw').text.strip()\n",
    "\n",
    "        quotes.append({'Quote': quote, 'Author': author, 'Type': quote_type})\n",
    "\n",
    "    df = pd.DataFrame(quotes)\n",
    "    return df\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://www.azquotes.com/'\n",
    "\n",
    "# Scrape the top quotes data and create a DataFrame\n",
    "df = scrape_top_quotes(url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51e46035",
   "metadata": {},
   "source": [
    "Q9.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_prime_ministers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', class_='content-table')\n",
    "    if table is None:\n",
    "        raise ValueError(\"Table element not found on the webpage\")\n",
    "    \n",
    "    rows = table.find_all('tr')[1:]\n",
    "    \n",
    "    prime_ministers = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        born_dead = columns[1].text.strip()\n",
    "        term_of_office = columns[2].text.strip()\n",
    "        remarks = columns[3].text.strip()\n",
    "        \n",
    "        prime_ministers.append({\n",
    "            'Name': name,\n",
    "            'Born-Dead': born_dead,\n",
    "            'Term of office': term_of_office,\n",
    "            'Remarks': remarks\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(prime_ministers)\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://www.jagranjosh.com/general-knowledge/list-of-prime-ministers-of-india-1424962594-1'\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "df = scrape_prime_ministers(url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d629bce9",
   "metadata": {},
   "source": [
    "Q10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_expensive_cars(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    cars = soup.find_all('article', class_='article')\n",
    "    \n",
    "    expensive_cars = []\n",
    "    for car in cars:\n",
    "        name = car.find('h3').text.strip()\n",
    "        price = car.find('span', class_='price-label').text.strip()\n",
    "        \n",
    "        expensive_cars.append({\n",
    "            'Car name': name,\n",
    "            'Price': price\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(expensive_cars)\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://www.motor1.com/features/358125/most-expensive-cars/'\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "df = scrape_expensive_cars(url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1e2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c275ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
